{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de8c49f-cddb-47a8-826b-2918df37b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a1e92e-0f58-437b-97f8-6157d8e28ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    PDF_DIRECTORY = \"papers/\"\n",
    "    VECTOR_DB_PATH = \"./vector_db\"\n",
    "    MODEL_NAME = \"llama3.2\"\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    SEARCH_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25dad11f-3feb-42ec-b7d5-48fc562d0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAssistant:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "        self.setup_embeddings()\n",
    "    \n",
    "    def setup_embeddings(self):\n",
    "        \"\"\"Configure the model embeddings\"\"\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.config.EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cuda'},  # Use 'cuda' if you have a GPU, otherwise use 'cpu'\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        print(f\"Embeddings configured: {self.config.EMBEDDING_MODEL}\")\n",
    "    \n",
    "    def load_vectorstore(self, use_chroma=True):\n",
    "        \"\"\"Load an existent vectorstore\"\"\"\n",
    "        if use_chroma:\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=self.config.VECTOR_DB_PATH,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore = FAISS.load_local(\n",
    "                self.config.VECTOR_DB_PATH, \n",
    "                self.embeddings, \n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "        print(\"Vector store loaded\")\n",
    "        return self.vectorstore\n",
    "    \n",
    "    def setup_qa_chain(self):\n",
    "        \"\"\"Configure the string question-answer\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Create or load the vectorstore first\")\n",
    "        \n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": self.config.SEARCH_K}\n",
    "        )\n",
    "        \n",
    "        llm = Ollama(\n",
    "            model=self.config.MODEL_NAME,\n",
    "            temperature=0.1,\n",
    "            num_ctx=4096\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        print(\"QA Chain configured correctly\")\n",
    "        return self.qa_chain\n",
    "    \n",
    "    def ask_question(self, question, verbose=False):\n",
    "        \"\"\"Make a question to the assistant\"\"\"\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"Configure QA chain first\")\n",
    "        \n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {result['result']}\")\n",
    "            print(\"\\nConsulted Sources:\")\n",
    "            for i, doc in enumerate(result['source_documents']):\n",
    "                print(f\"{i+1}. {doc.metadata['source_file']} - Page {doc.metadata.get('page', 'N/A')}\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def interactive_chat(assistant):\n",
    "    print(\"Interactive mode - Research assistant\")\n",
    "    print(\"Type 'quit' to exit or 'reset' to start a new conversation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nQuestion: \").strip()\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        if question.lower() == 'reset':\n",
    "            print(\"Conversation reset\")\n",
    "            continue\n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = assistant.ask_question(question, verbose=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8096908f-4317-48f1-9049-ed545a682e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gerardo\\AppData\\Local\\Temp\\ipykernel_8696\\3454659852.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings configured: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gerardo\\AppData\\Local\\Temp\\ipykernel_8696\\3454659852.py:20: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded\n",
      "QA Chain configured correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gerardo\\AppData\\Local\\Temp\\ipykernel_8696\\3454659852.py:43: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=Ollama(model='llama3.2', num_ctx=4096, temperature=0.1), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000023B0D09EC80>, search_kwargs={'k': 4}))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "\n",
    "# Initialize and Load\n",
    "assistant = ResearchAssistant(config)\n",
    "assistant.load_vectorstore(use_chroma=True)\n",
    "assistant.setup_qa_chain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b793702-30c6-4269-bcbd-1fdb2e4ab84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive mode - Research assistant\n",
      "Type 'quit' to exit or 'reset' to start a new conversation\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Does number of emitted photons depend on pump power?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Does number of emitted photons depend on pump power?\n",
      "Answer: Yes, the number of emitted photons does depend on pump power. According to the text, the emission rate at the highest pump power considered is 2.3 × 10^5 photon pairs per second for the pulsed-pump case and 5.7 × 10^8 photon pairs per second for the nondegenerate pulsed-pump regime, assuming the same average pump power and a repetition rate of 80 MHz. This suggests that an increase in pump power leads to an increase in the number of emitted photons.\n",
      "\n",
      "Consulted Sources:\n",
      "1. Conversion efficiency in the process of copolarized spontaneous four-wave mixing.pdf - Page 7\n",
      "2. Conversion efficiency in the process of copolarized spontaneous four-wave mixing.pdf - Page 7\n",
      "3. Energy correlations of photon pairs generated by a silicon microring resonator probed by Stimulated Four Wave Mixing.pdf - Page 3\n",
      "4. Counter-propagating spontaneous four wave mixing photon-pair factorability and ultra-narrowband single photons.pdf - Page 5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which materials are being used for research in photonic chips?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which materials are being used for research in photonic chips?\n",
      "Answer: The text mentions the following materials being used for research in photonic chips:\n",
      "\n",
      "1. Silicon\n",
      "2. III–V semiconductors\n",
      "3. Lithium niobate (LN)\n",
      "4. Silicon carbide (SiC)\n",
      "5. Nitrides\n",
      "6. Diamond\n",
      "7. Tantalum pentoxide (Ta2O5)\n",
      "\n",
      "Additionally, the text also mentions \"vð2Þ materials\" which are likely referring to second-order nonlinear materials, but it does not specify what these materials are.\n",
      "\n",
      "Consulted Sources:\n",
      "1. 2022 Roadmap on integrated quantum photonics.pdf - Page 9\n",
      "2. 2022 Roadmap on integrated quantum photonics.pdf - Page 7\n",
      "3. On-chip heralded single photon sources.pdf - Page 24\n",
      "4. 2022 Roadmap on integrated quantum photonics.pdf - Page 4\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  quit\n"
     ]
    }
   ],
   "source": [
    "interactive_chat(assistant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
